{"cells":[{"cell_type":"markdown","source":["# **TASK: Classification using Logistic Regression and parameters from Grid Search on preprocessed dataset with PAM50/LIMMA50 filters**"],"metadata":{"id":"6In_DsteVxhq"}},{"cell_type":"markdown","source":["Useful links:\n","\n","https://stackoverflow.com/questions/54608088/what-is-gridsearch-cv-results-could-any-explain-all-the-things-in-that-i-e-me\n","\n","https://python.plainenglish.io/how-to-use-pandas-profiling-on-google-colab-e34f34ff1c9f\n","\n","https://towardsdatascience.com/tuning-the-hyperparameters-of-your-machine-learning-model-using-gridsearchcv-7fc2bb76ff27\n","\n","https://rpubs.com/cliex159/884981\n","\n","https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","https://scikit-learn.org/stable/modules/linear_model.html#lasso\n"],"metadata":{"id":"g-2hxYh-jNtJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2369,"status":"ok","timestamp":1660902904545,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"},"user_tz":-120},"id":"WK4lbmG1OCIR","outputId":"f52f4297-5e31-4cba-8439-23f13eb9071f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"]}],"source":["# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/Drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0RV-_-TBQBkS"},"outputs":[],"source":["# Imports\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, balanced_accuracy_score, f1_score, matthews_corrcoef, classification_report, make_scorer\n","from sklearn.linear_model import LogisticRegression\n","import matplotlib.pyplot as plt\n","from xlwt import Workbook\n","from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","import os\n","from pandas_profiling import ProfileReport"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","!pip freeze\n","! pip install scikit-learn==0.24.2 # Downgrading the scikit learn library to obtain same results of previous experiments and Convergence"],"metadata":{"id":"9lmBEQvDgX2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9319,"status":"ok","timestamp":1660903046883,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"},"user_tz":-120},"id":"cUUSec6GPW7s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"882c67dc-f908-4939-9c79-fe77d73963cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["# Current working directory and other paths\n","cwd = os.getcwd()\n","print(cwd)\n","!cd Drive/\n","path = cwd + \"/Drive/My Drive/magistrale/BioinformaticsProject/data/\"\n","results_path = cwd + \"/Drive/My Drive/magistrale/BioinformaticsProject/results/\"\n","\n","# Count per Million matrix\n","cpm_dataset = pd.read_csv(path+\"CPM.csv\",index_col=0) #read the main CPM dataset(67k × 719)\n","cpm_dataset = cpm_dataset.transpose() # (719 × 67k)\n","# Training and Testing datasets\n","training_ds =  pd.read_excel(path+\"train.test.xlsx\", sheet_name=\"train\")\n","testing_ds = pd.read_excel(path+\"train.test.xlsx\", sheet_name=\"test\")\n","\n","# Feature space datesets\n","base_feature_space =path+\"FEATURE_SPACES(RAW +CPM).xlsx\"\n","# List of feature space name \n","feature_space_files =[\"FEATURE_SPACE1(PAM)\", \"FEATURE_SPACE2\",\"FEATURE_SPACE3(LIMMA)\",\"FEATURE_SPACE4(LIMMA)\", \"FEATURE_SPACE7(pamsimilarity)\",\"FEATURE_SPACE8(limmasimilarity)\"]"]},{"cell_type":"code","source":["def extract_and_reduce_by_columns(path, sheet_name, columns_ds, name):\n","  '''\n","      Function to extract dataset given a path, an excel sheet\n","  '''\n","  full_df = pd.read_excel(path, sheet_name=sheet_name) # path of subdatset \n","\n","  full_list= full_df['isoform'].values.tolist()  #exatrct the list of isoforms names as list\n","  data = cpm_dataset[np.intersect1d(cpm_dataset.columns, full_list)]  # find the mutual isoform between main datset and subdatset \n","  data.reset_index(inplace=True)\n","  data.rename(columns={ data.columns[0]: \"sample_id\" }, inplace = True)\n","\n","  x = columns_ds['sample_id'].values.tolist()\n","  data1= data.loc[data['sample_id'].isin(x)]\n","  result = pd.merge(data1, columns_ds, on='sample_id')\n","  result.rename(columns={'sample_id.1':'subtype'}, inplace=True )\n","  return result"],"metadata":{"id":"Gt0xqmGh_0lx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_w5ZpJ-d3W9_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660903059432,"user_tz":-120,"elapsed":10089,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"2685fee0-f8fa-4501-9eaf-47fb3a48bbb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train size: (550, 131)\n","X_test size: (137, 131)\n"]}],"source":["#---- Training Data import:\n","train = extract_and_reduce_by_columns(base_feature_space, feature_space_files[4], training_ds, 'trainingset') \n","X_train = train.drop([\"sample_id\",\"subtype\"],  axis = 1)\n","samples_train = train.sample_id\n","Y_train=train.subtype\n","print(\"X_train size:\", X_train.shape)\n","\n","#---- Testing Data import:\n","test = extract_and_reduce_by_columns(base_feature_space, feature_space_files[4], testing_ds, 'testingset') \n","X_test = test.drop([\"sample_id\",\"subtype\"], axis = 1)\n","Y_test = test.subtype\n","print(\"X_test size:\", X_test.shape)"]},{"cell_type":"code","source":["# profile = ProfileReport(train, title='Train Dataset', html={'style':{'full_width':True}})\n","# profile.to_notebook_iframe()"],"metadata":{"id":"MSKQ7l_DhHcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(\"X_train contains the samples in the train dataset: \", X_train)\n","# print(\"Y_train contains the labels in the train dataset: \", Y_train)\n","print(train.describe())"],"metadata":{"id":"eaWzBrtXSyoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cpw7DJlyRu-H"},"outputs":[],"source":["def write_results(titles, results, name_of_file):\n","      '''\n","      Function to write results metrics and confing into a csv file \n","      '''\n","      df = pd.DataFrame(results)\n","      df.to_csv(results_path+name_of_file + \".csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R0uTRLOSR1yy"},"outputs":[],"source":["def plot_confusion_matrix(y_true, y_pred, le, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    classes = classes[unique_labels(le.transform(y_true), le.transform(y_pred))]\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    # print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ND160_Y13rv7"},"outputs":[],"source":["# Configuration of parameters and names\n","\n","#RESULTS_file_name\n","fs= \"pam_similarity\"\n","cv_results_file_name = \"10-CV_results_Log_Reg\"\n","test_results_file_name = \"10-CV_TEST_Log_Reg\"\n","\n","#CV models attributes\n","#-10-folds:\n","column_titles_cv = [\"metric\", \"C\", \"l1_ratio\", \"mean_CVtest_score\", \"std_CVtest_score\",\n","                    \"fold_0_test_score\", \"fold_1_test_score\", \"fold_2_test_score\",\n","                    \"fold_3_test_score\", \"fold_4_test_score\", \"fold_5_test_score\",\n","                    \"fold_6_test_score\", \"fold_7_test_score\", \"fold_8_test_score\", \"fold_9_test_score\",\n","                    \"mean_fit_time\"]\n","\n","column_titles_test = [\"metric\", \"C\", \"l1_ratio\", \"10-f_CV\", \"TEST_Acc\", \"TEST_P\", \"TEST_R\", \"TEST_Ba\"]\n","metrics = [\"Balanced_accuracy\",\"Accuracy\"]\n","C = []\n","l1_ratio = []\n","cv_best = []\n","score_test_balanced_accuracy = []\n","score_test_accuracy = []\n","precision = []\n","recall = []\n","f1=[]\n","\n","#'dual':[False]\n","#GridSearch attributes\n","# Set the parameters by cross-validation\n","#'l1_ratio':[0.5], 'solver': ['saga'], 'penalty':['elasticnet']\n","\n","tuned_parameters = [{\n","    'multi_class':  ['ovr'],\n","    'penalty':['elasticnet'],\n","    'solver': ['saga'], \n","    'max_iter':[2000], \n","    'C':  [ 0.1], #[10 ** i for i in range(-2,1)],\n","    'l1_ratio': [0.1, 0.01] #[10 ** i for i in range(-2,1)] #'l1_ratio':[0.5]}]\n","    }]\n","\n","scores = [\"balanced_accuracy\",\"accuracy\"]\n","print(tuned_parameters)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tQlgjgAJSNMo"},"outputs":[],"source":["\n","\n","for index, score in enumerate(scores):\n","    # -------RESULTS in CROSS_VALIDATION-----------\n","    print(\"\\n\\n# Tuning hyper-parameters for %s\" % score)\n","\n","    # Fit and hyperparameter search\n","    clf = GridSearchCV(LogisticRegression(), tuned_parameters, scoring=score, cv=10)\n","    clf.fit(X_train, Y_train)\n","\n","    print(\"Best parameters set found on development set: \", clf.best_params_)\n","    # appending CV_results\n","    means = clf.cv_results_['mean_test_score']\n","    stds = clf.cv_results_['std_test_score']\n","    split_0_test_score = clf.cv_results_[\"split0_test_score\"]\n","    split_1_test_score = clf.cv_results_[\"split1_test_score\"]\n","    split_2_test_score = clf.cv_results_[\"split2_test_score\"]\n","    split_3_test_score = clf.cv_results_[\"split3_test_score\"]\n","    split_4_test_score = clf.cv_results_[\"split4_test_score\"]\n","    split_5_test_score = clf.cv_results_[\"split5_test_score\"]\n","    split_6_test_score = clf.cv_results_[\"split6_test_score\"]\n","    split_7_test_score = clf.cv_results_[\"split7_test_score\"]\n","    split_8_test_score = clf.cv_results_[\"split8_test_score\"]\n","    split_9_test_score = clf.cv_results_[\"split9_test_score\"]\n","\n","    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","        print(\"%0.3f (+/-%0.03f) for %r\"\n","              % (mean, std * 2, params))\n","\n","    # saving CV_test_scores\n","    # 10-folds\n","    cv_results = np.array([metrics[index], clf.cv_results_[\"param_C\"], clf.cv_results_[\"param_l1_ratio\"],\n","                        clf.cv_results_[\"mean_test_score\"], clf.cv_results_[\"std_test_score\"],\n","                           split_0_test_score, split_1_test_score, split_2_test_score, split_3_test_score, split_4_test_score,\n","                           split_5_test_score, split_6_test_score, split_7_test_score, split_8_test_score, split_9_test_score,\n","                           clf.cv_results_[\"mean_fit_time\"] ])\n","    \n","    write_results(results_path+ column_titles_cv, cv_results, cv_results_file_name + fs + \"-\" + metrics[index])\n","    print(\"printed\")\n","\n","     #--------TEST results-------\n","    print(\"Detailed classification report:\")\n","    print()\n","    print(\"The model is trained on the full development set.\")\n","    print(\"The scores are computed on the full evaluation set.\")\n","    print()\n","    y_true, y_pred = Y_test, clf.predict(X_test) # change here\n","    \n","    print()\n","    # bestCV_model results appending:\n","    C.append(clf.best_params_[\"C\"])\n","    l1_ratio.append(clf.best_params_[\"l1_ratio\"])\n","    cv_best.append(clf.best_score_)\n","    \n","\n","    score_test_balanced_accuracy.append(round(balanced_accuracy_score(y_true, y_pred), 3))\n","    score_test_accuracy.append(round(accuracy_score(y_true, y_pred), 3))\n","    precision.append(round(precision_score(y_true, y_pred, average=\"macro\"), 3))\n","    recall.append(round(recall_score(y_true, y_pred, average=\"macro\"), 3))\n","    f1.append(round(f1_score(y_true, y_pred, average=\"macro\"),3))\n","\n","    ## CONFUSION_MATRIX\n","    np.set_printoptions(precision=2)\n","    class_names = np.array([\"Basal\", \"Her2\", \"LumA\", \"LumB\", \"Normal\"])\n","\n","    le = LabelEncoder()\n","    le.fit(class_names)\n","    \n","    y_pred_train=clf.predict(X_train)\n","\n","    # on train set\n","    # Plot non-normalized confusion matrix\n","    plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names,\n","                          title='Confusion matrix on training set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_training-\" + metrics[index] + \".png\")\n","    # Plot normalized confusion matrix\n","    plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names, normalize=True,\n","                          title='Normalized confusion matrix on training set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_normalized_training-\" + metrics[index] + \".png\")\n","    \n","    # on test set\n","    # Plot non-normalized confusion matrix\n","    plot_confusion_matrix(Y_test, y_pred, le, classes=class_names,\n","                          title='Confusion matrix on testing set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_testing-\" + metrics[index] + \".png\")\n","    # Plot normalized confusion matrix\n","    plot_confusion_matrix(Y_test, y_pred, le, classes=class_names, normalize=True,\n","                          title='Normalized confusion matrix on testing set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_normalized_testing-\" + metrics[index] + \".png\")\n","\n","    # create a dataframe with training sample_id and y_pred_train and Y_train on columuns \n","    #(columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","    # title 'Predictions on training- from best' + metrics[index] +'.csv'\n","    pretraining = {'sample id' : train['sample_id'], 'predicted subtype' : y_pred_train, 'original subtypes' :Y_train} \n","    pretraining_dataframe= pd.DataFrame(pretraining)\n","    pretrainingcsv = pretraining_dataframe.to_csv('Predictions on training- from best' + metrics[index] + fs +'.csv')\n","\n","\n","    # create a dataframe with testing sample_id y_pred and Y_test on columuns \n","    # (columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","    # title 'Predictions on testing- from best' + metrics[index] +'.csv'\n","    pretesting = {'sample id' : test['sample_id'], 'predicted subtype' : y_pred, 'original subtypes' :Y_test} \n","    pretesting_dataframe= pd.DataFrame(pretesting)\n","    pretestingcsv = pretesting_dataframe.to_csv('Predictions on testing- from best' + metrics[index] + fs +'.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1660902976227,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"},"user_tz":-120},"id":"jXKFkMfjSO7i","outputId":"69c4d311-bdf8-4530-8431-790f3a083687"},"outputs":[{"output_type":"stream","name":"stdout","text":["saved\n","saved \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["test_results = np.array([metrics, C, l1_ratio, cv_best, score_test_accuracy,  precision, recall, score_test_balanced_accuracy])\n","write_results(column_titles_test, test_results,test_results_file_name )\n","print(\"saved \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHGxynhlxIos"},"outputs":[],"source":["np.set_printoptions(precision=2)\n","class_names = np.array([\"Basal\", \"Her2\", \"LumA\", \"LumB\", \"Normal\"])\n","\n","le = LabelEncoder()\n","le.fit(class_names)\n","    \n","y_pred_train=clf.predict(X_train)\n","# on train set\n","# Plot non-normalized confusion matrix\n","plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names,\n","                          title='Confusion matrix - from best' + metrics[0])\n","plt.savefig(\"/confusion_matrix-\" + metrics[0] + \".png\")\n","# Plot normalized confusion matrix\n","plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names, normalize=True,\n","                          title='Normalized confusion matrix- from best' + metrics[0])\n","plt.savefig(\"/confusion_matrix_normalized-\" + metrics[0] + \".png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcKW5oZgcv8U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660837907614,"user_tz":-120,"elapsed":259,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"a447a29c-020e-43ca-dce5-2279ed6f4894"},"outputs":[{"output_type":"stream","name":"stdout","text":["        sample id predicted subtype original subtypes\n","0    TCGA-3C-AALJ              LumB              LumB\n","1    TCGA-5T-A9QA              LumA              LumB\n","2    TCGA-A1-A0SF              LumA              LumA\n","3    TCGA-A1-A0SJ              LumA              LumA\n","4    TCGA-A1-A0SK             Basal             Basal\n","..            ...               ...               ...\n","132  TCGA-BH-A204              LumB              LumB\n","133  TCGA-BH-A208              LumA            Normal\n","134  TCGA-BH-A209              LumB              LumB\n","135  TCGA-BH-A42T              LumB              LumB\n","136  TCGA-C8-A3M7              LumA              LumA\n","\n","[137 rows x 3 columns]         sample id predicted subtype original subtypes\n","0    TCGA-3C-AAAU              LumA              LumA\n","1    TCGA-3C-AALI              Her2              Her2\n","2    TCGA-3C-AALK              LumA              LumA\n","3    TCGA-4H-AAAK              LumA              LumA\n","4    TCGA-5L-AAT0              LumA              LumA\n","..            ...               ...               ...\n","545  TCGA-C8-A27B             Basal             Basal\n","546  TCGA-C8-A3M8              LumB              LumB\n","547  TCGA-C8-A8HP              Her2              Her2\n","548  TCGA-C8-A8HQ              LumA              LumB\n","549  TCGA-C8-A8HR              LumA            Normal\n","\n","[550 rows x 3 columns]\n"]}],"source":["#create a dataframe with training sample_id and y_pred_train and Y_train on columuns \n","#(columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","# title 'Predictions on training- from best' + metrics[index] +'.csv'\n","pretraining = {'sample id' : train['sample_id'], 'predicted subtype' : y_pred_train, 'original subtypes' :Y_train} \n","pretraining_dataframe= pd.DataFrame(pretraining)\n","pretrainingcsv = pretraining_dataframe.to_csv('Predictions on training- from best.csv')\n","\n","#create a dataframe with testing sample_id y_pred and Y_test on columuns \n","#(columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","# title 'Predictions on testing- from best' + metrics[index] +'.csv'\n","pretesting = {'sample id' : test['sample_id'], 'predicted subtype' : y_pred, 'original subtypes' :Y_test} \n","pretesting_dataframe= pd.DataFrame(pretesting)\n","pretestingcsv = pretesting_dataframe.to_csv('Predictions on testing- from best.csv')\n","print(pretesting_dataframe,pretraining_dataframe)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9OulqRY_2jZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660838434881,"user_tz":-120,"elapsed":244,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"18dab617-c46c-44c8-998e-0dc267516086"},"outputs":[{"output_type":"stream","name":"stdout","text":["Balanced accuracy:  0.628\n","Accuracy:  0.81\n","Precision:  0.795\n","Recall:  0.628\n","F1 Score:  0.65\n"]}],"source":["# Single Train-Test Split Evaluation on model with tuned parameters\n","LogReg_trained = LogisticRegression(random_state=0, C=0.001,l1_ratio=0.1, multi_class = 'ovr', penalty= 'elasticnet',solver='saga', max_iter=1000).fit(X_train, Y_train)\n","\n","y_pred=LogReg_trained.predict(X_test)\n","print(\"Balanced accuracy: \", round(balanced_accuracy_score(Y_test, y_pred), 3))\n","print(\"Accuracy: \", round(accuracy_score(Y_test, y_pred), 3))\n","print(\"Precision: \", round(precision_score(Y_test, y_pred, average=\"macro\"), 3))\n","print(\"Recall: \",  round(recall_score(Y_test, y_pred, average=\"macro\"), 3)) \n","print(\"F1 Score: \", round(f1_score(Y_test, y_pred, average=\"macro\"), 3)) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkHoSNWN3tr_"},"outputs":[],"source":["b= pd.DataFrame(LogReg_trained.coef_, columns = X_train.columns )\n","b.to_csv(results_path+\"coef_limma_similairty.csv\")\n","\n","odds = np.exp(LogReg_trained.coef_)\n","b = pd.DataFrame(odds, columns=X_train.columns)\n","b.to_csv(results_path +\"coef_limma_similairty.csv\")"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"logistic_regression.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}