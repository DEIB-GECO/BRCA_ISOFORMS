{"cells":[{"cell_type":"markdown","source":["# **TASK: Classification using Lasso and parameters from Grid Search on preprocessed dataset with PAM50/LIMMA50 filters**\n","\n"],"metadata":{"id":"K5STry25bpZS"}},{"cell_type":"markdown","source":["Useful links:\n","\n","https://stackoverflow.com/questions/54608088/what-is-gridsearch-cv-results-could-any-explain-all-the-things-in-that-i-e-me\n","\n","https://python.plainenglish.io/how-to-use-pandas-profiling-on-google-colab-e34f34ff1c9f\n","\n","https://towardsdatascience.com/tuning-the-hyperparameters-of-your-machine-learning-model-using-gridsearchcv-7fc2bb76ff27\n","\n","https://rpubs.com/cliex159/884981\n","\n","https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","https://scikit-learn.org/stable/modules/linear_model.html#lasso\n"],"metadata":{"id":"g-2hxYh-jNtJ"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2176,"status":"ok","timestamp":1661086044755,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"},"user_tz":-120},"id":"WK4lbmG1OCIR","outputId":"93059f2e-4b29-4da0-e298-c4205806dfe4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/Drive; to attempt to forcibly remount, call drive.mount(\"/content/Drive\", force_remount=True).\n"]}],"source":["# mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/Drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"0RV-_-TBQBkS","executionInfo":{"status":"ok","timestamp":1661086047579,"user_tz":-120,"elapsed":1105,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}}},"outputs":[],"source":["# Imports\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, GridSearchCV\n","from sklearn.metrics import precision_score, recall_score, accuracy_score, balanced_accuracy_score, f1_score, matthews_corrcoef, classification_report, make_scorer\n","from sklearn.linear_model import LogisticRegression, Lasso\n","import matplotlib.pyplot as plt\n","from xlwt import Workbook\n","from sklearn.metrics import confusion_matrix\n","from sklearn.utils.multiclass import unique_labels\n","import os\n","from pandas_profiling import ProfileReport"]},{"cell_type":"markdown","source":[""],"metadata":{"id":"93TOb7ntblYF"}},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","# !pip freeze\n","# ! pip install scikit-learn==0.24.2 # Downgrading the scikit learn library to obtain same results of previous experiments and Convergence"],"metadata":{"id":"9lmBEQvDgX2J","executionInfo":{"status":"ok","timestamp":1661086094804,"user_tz":-120,"elapsed":360,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":13488,"status":"ok","timestamp":1661086109860,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"},"user_tz":-120},"id":"cUUSec6GPW7s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e953da04-4715-42d2-ca1e-467a1ac06ccf"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["# Current working directory and other paths\n","cwd = os.getcwd()\n","print(cwd)\n","!cd Drive/\n","path = cwd + \"/Drive/My Drive/magistrale/BioinformaticsProject/data/\"\n","results_path = cwd + \"/Drive/My Drive/magistrale/BioinformaticsProject/results/\"\n","\n","# Count per Million matrix\n","cpm_dataset = pd.read_csv(path+\"CPM.csv\",index_col=0) #read the main CPM dataset(67k × 719)\n","cpm_dataset = cpm_dataset.transpose() # (719 × 67k)\n","# Training and Testing datasets\n","training_ds =  pd.read_excel(path+\"train.test.xlsx\", sheet_name=\"train\")\n","testing_ds = pd.read_excel(path+\"train.test.xlsx\", sheet_name=\"test\")\n","\n","# Feature space datesets\n","base_feature_space =path+\"FEATURE_SPACES(RAW +CPM).xlsx\"\n","# List of feature space name \n","feature_space_files =[\"FEATURE_SPACE1(PAM)\", \"FEATURE_SPACE2\",\"FEATURE_SPACE3(LIMMA)\",\"FEATURE_SPACE4(LIMMA)\", \"FEATURE_SPACE7(pamsimilarity)\",\"FEATURE_SPACE8(limmasimilarity)\"]"]},{"cell_type":"code","source":["def extract_and_reduce_by_columns(path, sheet_name, columns_ds, name):\n","  '''\n","      Function to extract dataset given a path, an excel sheet\n","  '''\n","  full_df = pd.read_excel(path, sheet_name=sheet_name) # path of subdatset \n","\n","  full_list= full_df['isoform'].values.tolist()  #exatrct the list of isoforms names as list\n","  data = cpm_dataset[np.intersect1d(cpm_dataset.columns, full_list)]  # find the mutual isoform between main datset and subdatset \n","  data.reset_index(inplace=True)\n","  data.rename(columns={ data.columns[0]: \"sample_id\" }, inplace = True)\n","\n","  x = columns_ds['sample_id'].values.tolist()\n","  data1= data.loc[data['sample_id'].isin(x)]\n","  result = pd.merge(data1, columns_ds, on='sample_id')\n","  result.rename(columns={'sample_id.1':'subtype'}, inplace=True )\n"," \n","  # result.to_csv(name +\".csv\", index=False) # save as csv file \n","  return result"],"metadata":{"id":"Gt0xqmGh_0lx","executionInfo":{"status":"ok","timestamp":1661086166942,"user_tz":-120,"elapsed":257,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"_w5ZpJ-d3W9_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661086180802,"user_tz":-120,"elapsed":9607,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"0dd87545-f47a-47e9-ba81-12a4bd4e7cd1"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train size: (550, 131)\n","X_test size: (137, 131)\n"]}],"source":["#---- Training Data import:\n","train = extract_and_reduce_by_columns(base_feature_space, feature_space_files[4], training_ds, 'trainingset') \n","X_train = train.drop([\"sample_id\",\"subtype\"],  axis = 1)\n","samples_train = train.sample_id\n","Y_train=train.subtype\n","print(\"X_train size:\", X_train.shape)\n","\n","#---- Testing Data import:\n","test = extract_and_reduce_by_columns(base_feature_space, feature_space_files[4], testing_ds, 'testingset') \n","X_test = test.drop([\"sample_id\",\"subtype\"], axis = 1)\n","Y_test = test.subtype\n","print(\"X_test size:\", X_test.shape)"]},{"cell_type":"code","source":["# profile = ProfileReport(train, title='Train Dataset', html={'style':{'full_width':True}})\n","# profile.to_notebook_iframe()"],"metadata":{"id":"MSKQ7l_DhHcH","executionInfo":{"status":"ok","timestamp":1661086180803,"user_tz":-120,"elapsed":6,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# print(\"X_train contains the samples in the train dataset: \", X_train)\n","# print(\"Y_train contains the labels in the train dataset: \", Y_train)\n","print(train.describe())"],"metadata":{"id":"eaWzBrtXSyoE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661086181196,"user_tz":-120,"elapsed":398,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"da580a19-abe2-4c66-e61b-00dbce78374f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["       uc001cix.2  uc001ciy.2  uc001cmg.3   uc001ehz.2  uc001eib.2  \\\n","count  550.000000  550.000000  550.000000   550.000000  550.000000   \n","mean    40.408772    4.283931   23.951193    87.395602    0.414667   \n","std     45.014878    4.635825   22.140506   259.518210    1.250225   \n","min      1.314143    0.000000    0.000000     0.000000    0.000000   \n","25%     12.623505    1.419944    9.018923    20.304825    0.071127   \n","50%     24.727756    2.877836   17.319513    41.153053    0.178128   \n","75%     49.138632    5.359911   33.132701    89.217146    0.446732   \n","max    386.657840   46.912066  150.100004  5622.010706   26.436802   \n","\n","        uc001eic.2  uc001gcq.1  uc001gcr.1  uc001gxx.3  uc001hkm.2  ...  \\\n","count   550.000000  550.000000  550.000000  550.000000  550.000000  ...   \n","mean      8.096468   11.154435    7.648704   25.412759  136.939897  ...   \n","std      49.087039   11.158464    8.608324   20.456094  110.622174  ...   \n","min       0.000000    0.000000    0.000000    1.479666    8.685904  ...   \n","25%       0.730575    3.832749    2.445243   11.713809   59.327639  ...   \n","50%       2.092571    8.000932    4.960061   20.250957  108.556857  ...   \n","75%       5.279335   14.592556   10.043008   33.520923  183.743956  ...   \n","max    1088.834745   87.913277  100.566825  204.116002  965.734867  ...   \n","\n","       uc011kaz.1   uc011kco.1  uc011kvp.1  uc011kyl.1  uc011lpm.1  \\\n","count  550.000000   550.000000  550.000000  550.000000  550.000000   \n","mean     5.104531    24.319215    2.120568    8.320612    5.443529   \n","std      9.159963   121.363657    9.866933   18.125700    7.081644   \n","min      0.000000     0.017768    0.000000    0.000000    0.000000   \n","25%      0.000000     3.474733    0.000000    0.233373    1.473558   \n","50%      1.459955     7.957847    0.000000    1.440180    3.158703   \n","75%      5.492364    17.456091    0.323371    8.529283    6.712078   \n","max     72.834154  2256.724572  119.611247  168.822925   73.761550   \n","\n","       uc011lpn.1  uc011lpo.1  uc011lpp.1  uc011lpr.1  uc011lps.1  \n","count  550.000000  550.000000  550.000000  550.000000  550.000000  \n","mean     5.004276    0.493298    0.637168    0.867589    0.605615  \n","std      5.370624    0.601889    1.215665    1.256943    1.176995  \n","min      0.000000    0.000000    0.000000    0.000000    0.000000  \n","25%      1.482337    0.000000    0.000000    0.000000    0.000000  \n","50%      3.338269    0.307868    0.000000    0.383392    0.000000  \n","75%      6.577693    0.701349    0.880783    1.226158    0.730174  \n","max     44.811183    3.566705   12.558762    9.391812    8.394460  \n","\n","[8 rows x 131 columns]\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"Cpw7DJlyRu-H","executionInfo":{"status":"ok","timestamp":1661086181197,"user_tz":-120,"elapsed":3,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}}},"outputs":[],"source":["def write_results(titles, results, name_of_file):\n","      '''\n","      Function to write results metrics and confing into a csv file \n","      '''\n","      df = pd.DataFrame(results)\n","      df.to_csv(results_path+name_of_file + \".csv\")"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"R0uTRLOSR1yy","executionInfo":{"status":"ok","timestamp":1661086181851,"user_tz":-120,"elapsed":2,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}}},"outputs":[],"source":["def plot_confusion_matrix(y_true, y_pred, le, classes,\n","                          normalize=False,\n","                          title=None,\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if not title:\n","        if normalize:\n","            title = 'Normalized confusion matrix'\n","        else:\n","            title = 'Confusion matrix, without normalization'\n","\n","    # Compute confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","    # Only use the labels that appear in the data\n","    classes = classes[unique_labels(le.transform(y_true), le.transform(y_pred))]\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    # print(cm)\n","\n","    fig, ax = plt.subplots()\n","    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n","    ax.figure.colorbar(im, ax=ax)\n","    # We want to show all ticks...\n","    ax.set(xticks=np.arange(cm.shape[1]),\n","           yticks=np.arange(cm.shape[0]),\n","           # ... and label them with the respective list entries\n","           xticklabels=classes, yticklabels=classes,\n","           title=title,\n","           ylabel='True label',\n","           xlabel='Predicted label')\n","\n","    # Rotate the tick labels and set their alignment.\n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","    # Loop over data dimensions and create text annotations.\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, format(cm[i, j], fmt),\n","                    ha=\"center\", va=\"center\",\n","                    color=\"white\" if cm[i, j] > thresh else \"black\")\n","    fig.tight_layout()\n","    return ax"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"ND160_Y13rv7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661086693910,"user_tz":-120,"elapsed":271,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"32fa125e-1211-4a1a-ca24-828792918e52"},"outputs":[{"output_type":"stream","name":"stdout","text":["[{'multi_class': ['ovr'], 'penalty': ['l2'], 'solver': ['liblinear'], 'max_iter': [2000], 'dual': [True]}]\n"]}],"source":["# Configuration of parameters and names\n","\n","#RESULTS_file_name\n","fs= \"pam_similarity\"\n","cv_results_file_name = \"10-CV_results_Log_Reg\"\n","test_results_file_name = \"10-CV_TEST_Log_Reg\"\n","\n","#CV models attributes\n","#-10-folds:\n","column_titles_cv = [\"metric\", \"mean_CVtest_score\", \"std_CVtest_score\",\n","                    \"fold_0_test_score\", \"fold_1_test_score\", \"fold_2_test_score\",\n","                    \"fold_3_test_score\", \"fold_4_test_score\", \"fold_5_test_score\",\n","                    \"fold_6_test_score\", \"fold_7_test_score\", \"fold_8_test_score\", \"fold_9_test_score\",\n","                    \"mean_fit_time\"]\n","\n","column_titles_test = [\"metric\", \"10-f_CV\", \"TEST_Acc\", \"TEST_P\", \"TEST_R\", \"TEST_Ba\"]\n","metrics = [\"Balanced_accuracy\",\"Accuracy\"]\n","C = []\n","l1_ratio = []\n","cv_best = []\n","score_test_balanced_accuracy = []\n","score_test_accuracy = []\n","precision = []\n","recall = []\n","f1=[]\n","\n","#'dual':[False]\n","#GridSearch attributes\n","# Set the parameters by cross-validation\n","#'l1_ratio':[0.5], 'solver': ['saga'], 'penalty':['elasticnet']\n","\n","tuned_parameters = [{\n","    'multi_class':  ['ovr'],\n","    'penalty':['l2'],\n","    'solver': ['liblinear'], \n","    'max_iter':[2000],\n","    'dual': [True],\n","     # 'alpha':  [ 0.1, 0.01], #[10 ** i for i in range(-2,1)],\n","    # 'l1_ratio': [0.1, 0.01] #[10 ** i for i in range(-2,1)] #'l1_ratio':[0.5]}]\n","    }]\n","\n","scores = [\"balanced_accuracy\",\"accuracy\"]\n","print(tuned_parameters)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"tQlgjgAJSNMo","colab":{"base_uri":"https://localhost:8080/","height":457},"executionInfo":{"status":"error","timestamp":1661086768371,"user_tz":-120,"elapsed":31824,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"9b0b111d-668a-4f8c-f0e7-e3859645379d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","# Tuning hyper-parameters for balanced_accuracy\n","Best parameters set found on development set:  {'dual': True, 'max_iter': 2000, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}\n","0.721 (+/-0.182) for {'dual': True, 'max_iter': 2000, 'multi_class': 'ovr', 'penalty': 'l2', 'solver': 'liblinear'}\n","printed\n","Detailed classification report:\n","\n","The model is trained on the full development set.\n","The scores are computed on the full evaluation set.\n","\n"]},{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-0e04b6f63bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# bestCV_model results appending:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0ml1_ratio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"l1_ratio\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mcv_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'C'"]}],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","for index, score in enumerate(scores):\n","    # -------RESULTS in CROSS_VALIDATION-----------\n","    print(\"\\n\\n# Tuning hyper-parameters for %s\" % score)\n","\n","    # Fit and hyperparameter search\n","    clf = GridSearchCV(LogisticRegression(), tuned_parameters, scoring=score, cv=10)\n","    clf.fit(X_train, Y_train)\n","\n","    print(\"Best parameters set found on development set: \", clf.best_params_)\n","    # appending CV_results\n","    means = clf.cv_results_['mean_test_score']\n","    stds = clf.cv_results_['std_test_score']\n","    split_0_test_score = clf.cv_results_[\"split0_test_score\"]\n","    split_1_test_score = clf.cv_results_[\"split1_test_score\"]\n","    split_2_test_score = clf.cv_results_[\"split2_test_score\"]\n","    split_3_test_score = clf.cv_results_[\"split3_test_score\"]\n","    split_4_test_score = clf.cv_results_[\"split4_test_score\"]\n","    split_5_test_score = clf.cv_results_[\"split5_test_score\"]\n","    split_6_test_score = clf.cv_results_[\"split6_test_score\"]\n","    split_7_test_score = clf.cv_results_[\"split7_test_score\"]\n","    split_8_test_score = clf.cv_results_[\"split8_test_score\"]\n","    split_9_test_score = clf.cv_results_[\"split9_test_score\"]\n","\n","    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n","        print(\"%0.3f (+/-%0.03f) for %r\"\n","              % (mean, std * 2, params))\n","\n","    # saving CV_test_scores\n","    # 10-folds\n","    cv_results = np.array([metrics[index], \n","                        clf.cv_results_[\"mean_test_score\"], clf.cv_results_[\"std_test_score\"],\n","                           split_0_test_score, split_1_test_score, split_2_test_score, split_3_test_score, split_4_test_score,\n","                           split_5_test_score, split_6_test_score, split_7_test_score, split_8_test_score, split_9_test_score,\n","                           clf.cv_results_[\"mean_fit_time\"] ])\n","    \n","    # write_results(results_path+ column_titles_cv, cv_results, cv_results_file_name + fs + \"-\" + metrics[index])\n","    print(\"printed\")\n","\n","     #--------TEST results-------\n","    print(\"Detailed classification report:\")\n","    print()\n","    print(\"The model is trained on the full development set.\")\n","    print(\"The scores are computed on the full evaluation set.\")\n","    print()\n","    y_true, y_pred = Y_test, clf.predict(X_test) # change here\n","    \n","    # bestCV_model results appending:\n","    C.append(clf.best_params_[\"C\"])\n","    l1_ratio.append(clf.best_params_[\"l1_ratio\"])\n","    cv_best.append(clf.best_score_)\n","    \n","\n","    score_test_balanced_accuracy.append(round(balanced_accuracy_score(y_true, y_pred), 3))\n","    score_test_accuracy.append(round(accuracy_score(y_true, y_pred), 3))\n","    precision.append(round(precision_score(y_true, y_pred, average=\"macro\"), 3))\n","    recall.append(round(recall_score(y_true, y_pred, average=\"macro\"), 3))\n","    f1.append(round(f1_score(y_true, y_pred, average=\"macro\"),3))\n","\n","    ## CONFUSION_MATRIX\n","    np.set_printoptions(precision=2)\n","    class_names = np.array([\"Basal\", \"Her2\", \"LumA\", \"LumB\", \"Normal\"])\n","\n","    le = LabelEncoder()\n","    le.fit(class_names)\n","    \n","    y_pred_train=clf.predict(X_train)\n","\n","    # on train set\n","    # Plot non-normalized confusion matrix\n","    plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names,\n","                          title='Confusion matrix on training set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_training-\" + metrics[index] + \".png\")\n","    # Plot normalized confusion matrix\n","    plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names, normalize=True,\n","                          title='Normalized confusion matrix on training set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_normalized_training-\" + metrics[index] + \".png\")\n","    \n","    # on test set\n","    # Plot non-normalized confusion matrix\n","    plot_confusion_matrix(Y_test, y_pred, le, classes=class_names,\n","                          title='Confusion matrix on testing set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_testing-\" + metrics[index] + \".png\")\n","    # Plot normalized confusion matrix\n","    plot_confusion_matrix(Y_test, y_pred, le, classes=class_names, normalize=True,\n","                          title='Normalized confusion matrix on testing set- from best ' + metrics[index])\n","    plt.savefig(\"/confusion_matrix_normalized_testing-\" + metrics[index] + \".png\")\n","\n","    # create a dataframe with training sample_id and y_pred_train and Y_train on columuns \n","    #(columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","    # title 'Predictions on training- from best' + metrics[index] +'.csv'\n","    pretraining = {'sample id' : train['sample_id'], 'predicted subtype' : y_pred_train, 'original subtypes' :Y_train} \n","    pretraining_dataframe= pd.DataFrame(pretraining)\n","    pretrainingcsv = pretraining_dataframe.to_csv('Predictions on training- from best' + metrics[index] + fs +'.csv')\n","\n","\n","    # create a dataframe with testing sample_id y_pred and Y_test on columuns \n","    # (columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","    # title 'Predictions on testing- from best' + metrics[index] +'.csv'\n","    pretesting = {'sample id' : test['sample_id'], 'predicted subtype' : y_pred, 'original subtypes' :Y_test} \n","    pretesting_dataframe= pd.DataFrame(pretesting)\n","    pretestingcsv = pretesting_dataframe.to_csv('Predictions on testing- from best' + metrics[index] + fs +'.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":310,"status":"ok","timestamp":1660902976227,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"},"user_tz":-120},"id":"jXKFkMfjSO7i","outputId":"69c4d311-bdf8-4530-8431-790f3a083687"},"outputs":[{"output_type":"stream","name":"stdout","text":["saved\n","saved \n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  \"\"\"Entry point for launching an IPython kernel.\n"]}],"source":["test_results = np.array([metrics, C, l1_ratio, cv_best, score_test_accuracy,  precision, recall, score_test_balanced_accuracy])\n","write_results(column_titles_test, test_results,test_results_file_name )\n","print(\"saved \")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHGxynhlxIos"},"outputs":[],"source":["np.set_printoptions(precision=2)\n","class_names = np.array([\"Basal\", \"Her2\", \"LumA\", \"LumB\", \"Normal\"])\n","\n","le = LabelEncoder()\n","le.fit(class_names)\n","    \n","y_pred_train=clf.predict(X_train)\n","# on train set\n","# Plot non-normalized confusion matrix\n","plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names,\n","                          title='Confusion matrix - from best' + metrics[0])\n","plt.savefig(\"/confusion_matrix-\" + metrics[0] + \".png\")\n","# Plot normalized confusion matrix\n","plot_confusion_matrix(Y_train, y_pred_train, le, classes=class_names, normalize=True,\n","                          title='Normalized confusion matrix- from best' + metrics[0])\n","plt.savefig(\"/confusion_matrix_normalized-\" + metrics[0] + \".png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JcKW5oZgcv8U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660837907614,"user_tz":-120,"elapsed":259,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"a447a29c-020e-43ca-dce5-2279ed6f4894"},"outputs":[{"output_type":"stream","name":"stdout","text":["        sample id predicted subtype original subtypes\n","0    TCGA-3C-AALJ              LumB              LumB\n","1    TCGA-5T-A9QA              LumA              LumB\n","2    TCGA-A1-A0SF              LumA              LumA\n","3    TCGA-A1-A0SJ              LumA              LumA\n","4    TCGA-A1-A0SK             Basal             Basal\n","..            ...               ...               ...\n","132  TCGA-BH-A204              LumB              LumB\n","133  TCGA-BH-A208              LumA            Normal\n","134  TCGA-BH-A209              LumB              LumB\n","135  TCGA-BH-A42T              LumB              LumB\n","136  TCGA-C8-A3M7              LumA              LumA\n","\n","[137 rows x 3 columns]         sample id predicted subtype original subtypes\n","0    TCGA-3C-AAAU              LumA              LumA\n","1    TCGA-3C-AALI              Her2              Her2\n","2    TCGA-3C-AALK              LumA              LumA\n","3    TCGA-4H-AAAK              LumA              LumA\n","4    TCGA-5L-AAT0              LumA              LumA\n","..            ...               ...               ...\n","545  TCGA-C8-A27B             Basal             Basal\n","546  TCGA-C8-A3M8              LumB              LumB\n","547  TCGA-C8-A8HP              Her2              Her2\n","548  TCGA-C8-A8HQ              LumA              LumB\n","549  TCGA-C8-A8HR              LumA            Normal\n","\n","[550 rows x 3 columns]\n"]}],"source":["#create a dataframe with training sample_id and y_pred_train and Y_train on columuns \n","#(columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","# title 'Predictions on training- from best' + metrics[index] +'.csv'\n","pretraining = {'sample id' : train['sample_id'], 'predicted subtype' : y_pred_train, 'original subtypes' :Y_train} \n","pretraining_dataframe= pd.DataFrame(pretraining)\n","pretrainingcsv = pretraining_dataframe.to_csv('Predictions on training- from best.csv')\n","\n","#create a dataframe with testing sample_id y_pred and Y_test on columuns \n","#(columns names>> sample id, predicted subtype, original subtypes)and save it as a csv file with\n","# title 'Predictions on testing- from best' + metrics[index] +'.csv'\n","pretesting = {'sample id' : test['sample_id'], 'predicted subtype' : y_pred, 'original subtypes' :Y_test} \n","pretesting_dataframe= pd.DataFrame(pretesting)\n","pretestingcsv = pretesting_dataframe.to_csv('Predictions on testing- from best.csv')\n","print(pretesting_dataframe,pretraining_dataframe)"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"K9OulqRY_2jZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1661086895039,"user_tz":-120,"elapsed":5635,"user":{"displayName":"arianna galzerano","userId":"01686885889235545991"}},"outputId":"d163d9ce-0725-4fb9-a50c-069710680aee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Balanced accuracy:  0.598\n","Accuracy:  0.752\n","Precision:  0.649\n","Recall:  0.598\n","F1 Score:  0.615\n"]}],"source":["# Single Train-Test Split Evaluation on model with tuned parameters\n","\n","LogReg_trained = LogisticRegression(random_state=0, dual=True, max_iter=2000, multi_class='ovr', penalty='l2', solver='liblinear').fit(X_train, Y_train)\n","\n","y_pred=LogReg_trained.predict(X_test)\n","print(\"Balanced accuracy: \", round(balanced_accuracy_score(Y_test, y_pred), 3))\n","print(\"Accuracy: \", round(accuracy_score(Y_test, y_pred), 3))\n","print(\"Precision: \", round(precision_score(Y_test, y_pred, average=\"macro\"), 3))\n","print(\"Recall: \",  round(recall_score(Y_test, y_pred, average=\"macro\"), 3)) \n","print(\"F1 Score: \", round(f1_score(Y_test, y_pred, average=\"macro\"), 3)) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KkHoSNWN3tr_"},"outputs":[],"source":["b= pd.DataFrame(LogReg_trained.coef_, columns = X_train.columns )\n","b.to_csv(results_path+\"coef_limma_similairty.csv\")\n","\n","odds = np.exp(LogReg_trained.coef_)\n","b = pd.DataFrame(odds, columns=X_train.columns)\n","b.to_csv(results_path +\"coef_limma_similairty.csv\")"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"logistic_regression_with_regularization.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}